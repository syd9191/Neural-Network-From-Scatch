# Neural Networks From Scratch

The notebook is a comprehensive guide to building a neural network from scratch, following a structured approach, and also served as a documentation of my learning journey through key concepts and math behind neural networks.

## 1.Introduction and Building Blocks:

The notebook begins with an introduction to neural networks, specifically neurons, and demonstrates how to calculate the output of a single neuron using basic Python lists and arithmetic operations.

## 2.Neural Network Layers:

Examples are provided for calculating the output of a neural network layer manually, followed by dynamically handling the process using functions.

## 3.Numpy for Optimization:

The notebook transitions to using numpy for efficient matrix operations, which simplifies the handling of larger neural network layers and batch processing.


## 4.Dataset and Activation Functions:

A custom spiral dataset is generated for training. The ReLU (Rectified Linear Unit) activation function is implemented, followed by a full implementation of a dense layer.

## 5.Softmax and Loss Function:

The Softmax activation function and a loss function are introduced for multi-class classification tasks.

## 6.Training with MNIST:

The MNIST dataset is loaded to demonstrate how the network is trained on a real-world problem. Gradient descent is used to optimize the weights of the network.
Accuracy Evaluation:

The final section evaluates the network’s performance on the test set and prints the accuracy of predicting 10,000 digits from the MNIST dataset.
This notebook effectively walks through the entire process of building a simple neural network from basic concepts to implementation, using numpy for efficient calculations, and concludes with real-world testing on the MNIST dataset. ​

